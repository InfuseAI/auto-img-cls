{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "auto-img-cls.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "yxOYA7t2ZVqw"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Step 0: Init the AutoML program\n",
        "\n",
        "Run the cell to import necessary modules and training code."
      ],
      "metadata": {
        "id": "yxOYA7t2ZVqw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install Comet"
      ],
      "metadata": {
        "id": "PNEAAyHgzqQh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install comet_ml"
      ],
      "metadata": {
        "id": "JyC_YdBuzk0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define ImageClassifier class"
      ],
      "metadata": {
        "id": "5VP9SZwG0AJI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0u9W76KIZKGx"
      },
      "outputs": [],
      "source": [
        "# import comet_ml at the top of your file\n",
        "from comet_ml import Experiment, ExistingExperiment, Artifact, Optimizer\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import contextlib\n",
        "import tensorflow as tf\n",
        "import tempfile\n",
        "import shutil\n",
        "import zipfile\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import zipfile\n",
        "\n",
        "@contextlib.contextmanager\n",
        "def _tempdir():\n",
        "    dirpath = tempfile.mkdtemp()\n",
        "    yield dirpath\n",
        "    shutil.rmtree(dirpath)\n",
        "\n",
        "\n",
        "class ImageClassifier:\n",
        "    def __init__(self, experiment):\n",
        "        self.model = None\n",
        "        self.image_size = None\n",
        "        self.class_names = None\n",
        "        self.experiment = experiment\n",
        "\n",
        "    def train(self, dataset_dir, batch_size=32, epochs=1, image_size=(160, 160), learning_rate=0.0001):\n",
        "\n",
        "        #params = {\n",
        "        #    'epochs': epochs,\n",
        "        #    'batch_size': batch_size,\n",
        "        #    'learning_rate': learning_rate,\n",
        "        #}\n",
        "\n",
        "        #self.experiment.log_parameters(params)\n",
        "\n",
        "        # Step1: Data prep\n",
        "        train_dataset = tf.keras.utils.image_dataset_from_directory(dataset_dir,\n",
        "                                                                    seed=1337,\n",
        "                                                                    validation_split=0.2,\n",
        "                                                                    batch_size=batch_size,\n",
        "                                                                    subset='training',\n",
        "                                                                    image_size=image_size)\n",
        "        validation_dataset = tf.keras.utils.image_dataset_from_directory(dataset_dir,\n",
        "                                                                         seed=1337,\n",
        "                                                                         validation_split=0.2,\n",
        "                                                                         batch_size=batch_size,\n",
        "                                                                         subset='validation',\n",
        "                                                                         image_size=image_size)\n",
        "        class_names = train_dataset.class_names\n",
        "\n",
        "        print('Number of trian batches: %d' %\n",
        "              tf.data.experimental.cardinality(train_dataset))\n",
        "        print('Number of validation batches: %d' %\n",
        "              tf.data.experimental.cardinality(validation_dataset))\n",
        "        train_dataset = train_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "        validation_dataset = validation_dataset.prefetch(\n",
        "            buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "        # Step2: Model Architecture\n",
        "\n",
        "        # Skip the data_augmentation because\n",
        "        # https://stackoverflow.com/questions/69955838/saving-model-on-tensorflow-2-7-0-with-data-augmentation-layer\n",
        "        #\n",
        "        # data_augmentation = tf.keras.Sequential([\n",
        "        #     tf.keras.layers.RandomFlip('horizontal'),\n",
        "        #     tf.keras.layers.RandomRotation(0.2),\n",
        "        # ])\n",
        "        preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\n",
        "        image_shape = image_size + (3,)\n",
        "        base_model = tf.keras.applications.MobileNetV2(input_shape=image_shape,\n",
        "                                                       include_top=False,\n",
        "                                                       weights='imagenet')\n",
        "        base_model.trainable = True\n",
        "        fine_tune_at = 100\n",
        "        for layer in base_model.layers[:fine_tune_at]:\n",
        "            layer.trainable = False\n",
        "\n",
        "        global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
        "        inputs = tf.keras.Input(shape=image_shape)\n",
        "        x = inputs\n",
        "        # x = data_augmentation(x)\n",
        "        x = preprocess_input(x)\n",
        "        x = base_model(x, training=False)\n",
        "        x = global_average_layer(x)\n",
        "        x = tf.keras.layers.Dropout(0.2)(x)\n",
        "        x = tf.keras.layers.Dense(len(class_names))(x)\n",
        "        outputs = tf.nn.softmax(x)\n",
        "        model = tf.keras.Model(inputs, outputs)\n",
        "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "                      loss=tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "            from_logits=False),\n",
        "            metrics=['accuracy'])\n",
        "        model.summary()\n",
        "\n",
        "        # Step 3: Start training\n",
        "        history = model.fit(train_dataset,\n",
        "                            epochs=epochs,\n",
        "                            validation_data=validation_dataset)\n",
        "\n",
        "        # Step 4: Evaluation\n",
        "        if validation_dataset:\n",
        "            loss, accuracy = model.evaluate(validation_dataset)\n",
        "            print('Test accuracy :', accuracy)\n",
        "\n",
        "            # Retrieve a batch of images from the test set\n",
        "            image_batch, label_batch = validation_dataset.as_numpy_iterator().next()\n",
        "            predictions = model.predict_on_batch(image_batch)\n",
        "\n",
        "            # Apply softmax and argmax to find the most possible class\n",
        "            predictions = model.predict_on_batch(image_batch)\n",
        "            predictions = tf.math.argmax(predictions, axis=-1)\n",
        "            print('Predictions:\\n', predictions.numpy())\n",
        "            print('Labels:\\n', label_batch)\n",
        "\n",
        "        self.model = model\n",
        "        self.class_names = class_names\n",
        "        self.image_size = (160, 160)\n",
        "        image_size = (160, 160)\n",
        "\n",
        "        # try:\n",
        "        #     artifact = Artifact(dataset_dir, 'dataset')\n",
        "        #     artifact.add('{}.zip'.format(dataset_dir))\n",
        "\n",
        "        #     self.experiment.log_artifact(artifact)\n",
        "        # except Exception as e:\n",
        "        #     print('Error: ')\n",
        "        #     print(e)\n",
        "        # finally:\n",
        "        #     self.experiment.end()\n",
        "        self.experiment.end()\n",
        "\n",
        "        return history\n",
        "\n",
        "    def upload_image(self, img, name):\n",
        "        exp = ExistingExperiment(\n",
        "            api_key=self.experiment.api_key,\n",
        "            project_name=self.experiment.project_name,\n",
        "            workspace=self.experiment.workspace,\n",
        "            previous_experiment=self.experiment.get_key(),\n",
        "        )\n",
        "        exp.log_image(img, name=name)\n",
        "        exp.end()\n",
        "\n",
        "    def save(self, modelfile):\n",
        "        with _tempdir() as modelpath:\n",
        "            self.model.save(modelpath)\n",
        "            with open(f'{modelpath}/class_names.txt', 'w') as f:\n",
        "                for class_name in self.class_names:\n",
        "                    print(class_name, file=f)\n",
        "\n",
        "            exp = ExistingExperiment(\n",
        "                api_key=self.experiment.api_key,\n",
        "                project_name=self.experiment.project_name,\n",
        "                workspace=self.experiment.workspace,\n",
        "                previous_experiment=self.experiment.get_key(),\n",
        "            )\n",
        "            exp.log_model(\"model-1\", modelpath, metadata={\"m1\":\"v1\", \"m2\":[1,2,3]})\n",
        "            exp.end()\n",
        "\n",
        "            with zipfile.ZipFile(modelfile, 'w', zipfile.ZIP_DEFLATED) as zf:\n",
        "                for dirname, subdirs, files in os.walk(modelpath):\n",
        "                    arc_dirname = dirname[len(modelpath):]\n",
        "                    print(f'dir : {arc_dirname}/')\n",
        "                    zf.write(dirname, arc_dirname)\n",
        "                    for filename in files:\n",
        "                        print(f'file: {arc_dirname}/{filename}')\n",
        "                        zf.write(os.path.join(dirname, filename),\n",
        "                                 os.path.join(arc_dirname, filename))\n",
        "\n",
        "    def load(self, modelfile):\n",
        "        with _tempdir() as dirpath:\n",
        "            with zipfile.ZipFile(modelfile, 'r') as zip_ref:\n",
        "                zip_ref.extractall(dirpath)\n",
        "            model = tf.keras.models.load_model(dirpath)\n",
        "            with open(f\"{dirpath}/class_names.txt\") as f:\n",
        "                class_names = f.readlines()\n",
        "            class_names = [class_name.strip() for class_name in class_names]\n",
        "\n",
        "        self.image_size = (160, 160)\n",
        "        self.class_names = (class_names)\n",
        "        self.model = model\n",
        "\n",
        "    def predict(self, img_path):\n",
        "        with Image.open(img_path) as image:\n",
        "            image = image.resize(self.image_size).convert(\"RGB\")\n",
        "            x = tf.keras.preprocessing.image.img_to_array(image)\n",
        "            x = tf.expand_dims(x, 0)\n",
        "        result = self.model(x)\n",
        "        result = tf.squeeze(result)\n",
        "        cls_idx = int(tf.math.argmax(result, axis=-1))\n",
        "        cls = self.class_names[cls_idx]\n",
        "        return (cls, result.numpy())\n",
        "\n",
        "    def predict_img(self, image):\n",
        "        image = image.resize(self.image_size).convert(\"RGB\")\n",
        "        x = tf.keras.preprocessing.image.img_to_array(image)\n",
        "        x = tf.expand_dims(x, 0)\n",
        "        result = self.model(x)\n",
        "        result = tf.squeeze(result)\n",
        "        cls_idx = int(tf.math.argmax(result, axis=-1))\n",
        "        cls = self.class_names[cls_idx]\n",
        "        return (cls, result.numpy())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Prepare the dataset\n",
        "\n",
        "The dataset should be a 'zip' file with the following format.\n",
        "\n",
        "```\n",
        "flowers_photos/\n",
        "  daisy/\n",
        "  dandelion/\n",
        "  roses/\n",
        "  sunflowers/\n",
        "  tulips/\n",
        "```  \n",
        "\n",
        "Please see the document in tensorflow\n",
        "https://www.tensorflow.org/tutorials/load_data/images\n",
        "\n",
        "\n",
        "There are two way to prepare the dataset\n",
        "\n",
        "1. Download the example dataset\n",
        "1. Use your own dataset\n",
        "\n",
        "After the step 1, the variable of `dataset_dir` should be set as the directory of your dataset."
      ],
      "metadata": {
        "id": "RZjE7iZbZhx7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Option 1: Download the example dataset\n",
        "\n",
        "If you would like to have a quick try, you can use the example dataset."
      ],
      "metadata": {
        "id": "-eFB1gChYEV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_DATASET_URL = 'https://storage.googleapis.com/infuseai-auto-img-cls/datasets/noodles_v1.2.zip'\n",
        "\n",
        "!wget $_DATASET_URL\n",
        "\n",
        "import os\n",
        "import tempfile\n",
        "\n",
        "basename = os.path.basename(_DATASET_URL)\n",
        "\n",
        "with zipfile.ZipFile(basename, 'r') as zip_ref:\n",
        "    zip_ref.extractall(\".\")\n",
        "\n",
        "base, ext = os.path.splitext(basename)\n",
        "dataset_dir = f\"{base}\"\n",
        "print(f\"dataset dir: {dataset_dir}\")"
      ],
      "metadata": {
        "id": "Ya6nXUaAuUwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Option 2: Use your own dataset"
      ],
      "metadata": {
        "id": "jKZ8ClOkZ6Ta"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you would like to prepare your special dataset, you can use the project [simple_image_download](https://github.com/RiddlerQ/simple_image_download) to download images from google search. But it seems there are some problem if we run it in colab, I recommend to prepare the dataset in your local machine, and upload to colab.\n",
        "\n",
        "1. Clone the auto-img-cls\n",
        "   ```\n",
        "   git clone git@github.com:InfuseAI/auto-img-cls.git\n",
        "   cd auto-img-cls\n",
        "   ```\n",
        "1. Install `simple_image_download`\n",
        "   ```\n",
        "   pip install simple_image_download\n",
        "   ```\n",
        "1. Edit the `download.py`. Change the keywords of images you would like to download\n",
        "   ```\n",
        "   #download.py\n",
        "   from simple_image_download import simple_image_download as simp\n",
        "\n",
        "   response = simp.simple_image_download\n",
        "   response().download('apple,banana,guava', 50)\n",
        "   ```\n",
        "1. Run python script\n",
        "   ```\n",
        "   python ./download\n",
        "   ```\n",
        "  \n",
        "   The downloaded image will go to `simple_images` folder.\n",
        "1. rename, zip, and upload to colab\n",
        "1. Run the below code cell to unzip and set the variable `dataset_dir`"
      ],
      "metadata": {
        "id": "QUeQ1iHQz_0_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_DATASET_FILE = '/content/cats_and_dogs_50.zip'\n",
        "\n",
        "import os\n",
        "import tempfile\n",
        "\n",
        "with zipfile.ZipFile(_DATASET_FILE, 'r') as zip_ref:\n",
        "    zip_ref.extractall(\".\")\n",
        "\n",
        "basename = os.path.basename(_DATASET_FILE)\n",
        "dataset_dir, ext = os.path.splitext(basename)\n",
        "\n",
        "print(f\"dataset dir: {dataset_dir}\")"
      ],
      "metadata": {
        "id": "VL6P_suOZ5SK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6192166-67d0-473f-a6c4-eb9f1f42469f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset dir: cats_and_dogs_50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Train\n",
        "\n",
        "Now, we have dataset. Then we will start to train a model from the dataset.\n",
        "\n",
        "Most of the time, the step should not be a single run, we should adjust the parameters (or said hyperparameter) to get the best result.\n",
        "\n",
        "There are three parameter we can adjust\n",
        "1. `EPOCHS`: how many times we should go through the whole dataset\n",
        "2. `BATCH_SIZE`: how many data we should update the weights\n",
        "3. `LEARNING_RATE`: how big step we should update the weights for each batch\n",
        "\n",
        "In the training process, we will split the dataset into two part\n",
        "- Training set (80%): use to train and update the weight\n",
        "- Validation set (20%): use to validate the model\n",
        "\n",
        "The goal for each experiment is to get the best accuracy in the validation dataset."
      ],
      "metadata": {
        "id": "Em5mHPGMeR4q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_dir = 'noodles_v1.2'"
      ],
      "metadata": {
        "id": "Ofps_ZOW3wGt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EPOCHS = 2\n",
        "# BATCH_SIZE = 20\n",
        "# LEARNING_RATE = 0.0001\n",
        "\n",
        "config = {\n",
        "    # We pick the Bayes algorithm:\n",
        "    \"algorithm\": \"bayes\",\n",
        "\n",
        "    # Declare your hyperparameters in the Vizier-inspired format:\n",
        "    \"parameters\": {\n",
        "        \"epochs\": {\"type\": \"integer\", \"min\": 4, \"max\": 5},\n",
        "        \"batch_size\": {\"type\": \"integer\", \"min\": 20, \"max\": 25},\n",
        "        \"learning_rate\": {\"type\": \"discrete\", \"values\": [0.1, 0.01, 0.001]}\n",
        "    },\n",
        "    # \"learning_rate\": {\"type\": \"float\", \"min\": 0.0001, \"max\": 0.001, \"scalingType\": \"loguniform\"},\n",
        "\n",
        "    # Declare what we will be optimizing, and how:\n",
        "    \"spec\": {\n",
        "        \"metric\": \"val_accuracy\",\n",
        "        \"objective\": \"maximum\",\n",
        "    },\n",
        "}\n",
        "opt = Optimizer(config, api_key='<your_api_key>')\n",
        "\n",
        "# Create an experiment with your api key\n",
        "#experiment = Experiment(\n",
        "#    api_key='<your_api_key>,',\n",
        "#    project_name='<your_project>',\n",
        "#    workspace='<your_workspace>',\n",
        "#    auto_metric_step_rate=1,\n",
        "#)\n",
        "\n",
        "for experiment in opt.get_experiments(\n",
        "        project_name='<your_project>',\n",
        "        workspace='<your_workspace>',\n",
        "        ):\n",
        "    classifier = ImageClassifier(experiment)\n",
        "    epochs = experiment.get_parameter(\"epochs\")\n",
        "    batch_size = experiment.get_parameter(\"batch_size\")\n",
        "    learning_rate = experiment.get_parameter(\"learning_rate\")\n",
        "    history = classifier.train(dataset_dir, epochs=epochs, batch_size=batch_size, learning_rate=learning_rate)\n",
        "    #history = classifier.train(dataset_dir, epochs=EPOCHS, batch_size=BATCH_SIZE, learning_rate=LEARNING_RATE)"
      ],
      "metadata": {
        "id": "1TVF7a28aPOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Predict\n",
        "\n",
        "Use an image to test the model"
      ],
      "metadata": {
        "id": "5bCdXrro3qYD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as img\n",
        "\n",
        "_IMG_PATH=\"/content/noodles_v1.2/beef_noodle/beef_noodle_1.jpeg\"\n",
        "\n",
        "image = img.imread(_IMG_PATH)\n",
        "cls, probs = classifier.predict(_IMG_PATH)\n",
        "\n",
        "plt.title(cls)\n",
        "plt.imshow(image)\n",
        "plt.show()\n",
        "\n",
        "print(f\"predict: {cls}\")\n",
        "print(f\"class_names: {classifier.class_names}\")\n",
        "print(f\"probability: {probs}\")\n",
        "\n",
        "classifier.upload_image(image, 'beef_noodle_1.jpeg')"
      ],
      "metadata": {
        "id": "EBgrIgW0mIkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Save and Load\n",
        "\n",
        "After several experiments, we can pick a best model to save."
      ],
      "metadata": {
        "id": "d_UHaf1s4f4i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save the model"
      ],
      "metadata": {
        "id": "rlaJHysU4kg6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_MODEL_FILE=\"model.zip\"\n",
        "\n",
        "classifier.save(_MODEL_FILE)"
      ],
      "metadata": {
        "id": "DGxQObmh46so"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the model"
      ],
      "metadata": {
        "id": "x8UCj0Pv4mcv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_MODEL_FILE=\"model.zip\"\n",
        "\n",
        "classifier=ImageClassifier()\n",
        "classifier.load(_MODEL_FILE)\n"
      ],
      "metadata": {
        "id": "iuQZoaLBzrCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Package and deploy the model"
      ],
      "metadata": {
        "id": "FEmIz1eq2YJr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Download the model to your local machine\n",
        "2. The model name should be `model.zip`\n",
        "3. Build the docker image\n",
        "   ```\n",
        "   docker build --tag model_noodles .\n",
        "   ```\n",
        "3. Run the docker image\n",
        "   ```\n",
        "   docker run -p 8501:8501 model_noodles\n",
        "   ```\n",
        "5. Check http://localhost:8501. \n",
        "6. Cheers!! You have your own image classifier!!\n",
        "\n",
        "\n",
        "If you use mac m1 to run tensorflow docker image, currently there are [known issue](https://github.com/tensorflow/tensorflow/issues/52845) that would crash."
      ],
      "metadata": {
        "id": "R5fi2J4H2zDx"
      }
    }
  ]
}

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "auto-img-cls.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "yxOYA7t2ZVqw"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Init the AutoML program"
      ],
      "metadata": {
        "id": "yxOYA7t2ZVqw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0u9W76KIZKGx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "def train(train_dir, validation_dir=None, batch_size=32, initial_epachs=1, finetune_epochs=1, image_size=(160,160)):\n",
        "    train_dataset = tf.keras.utils.image_dataset_from_directory(train_dir,\n",
        "                                                            shuffle=True,\n",
        "                                                            batch_size=batch_size,\n",
        "                                                            image_size=image_size)\n",
        "    print(train_dataset)\n",
        "\n",
        "    if validation_dir:\n",
        "      validation_dataset = tf.keras.utils.image_dataset_from_directory(validation_dir,\n",
        "                                                                  shuffle=True,\n",
        "                                                                  batch_size=batch_size,\n",
        "                                                                  image_size=image_size)\n",
        "      batches = tf.data.experimental.cardinality(validation_dataset)\n",
        "      if batches == 1:\n",
        "        test_dataset = validation_dataset\n",
        "      elif batches < 5:\n",
        "        test_dataset = validation_dataset.take(batches // 2)\n",
        "        validation_dataset = validation_dataset.skip(batches // 2)\n",
        "      else:\n",
        "        test_dataset = validation_dataset.take(batches // 5)\n",
        "        validation_dataset = validation_dataset.skip(batches // 5)\n",
        "    else:\n",
        "      test_dataset = None\n",
        "      validation_dataset = None\n",
        "\n",
        "    print('Number of trian batches: %d' % tf.data.experimental.cardinality(train_dataset))\n",
        "    if validation_dataset:\n",
        "      print('Number of validation batches: %d' % tf.data.experimental.cardinality(validation_dataset))\n",
        "      print('Number of test batches: %d' % tf.data.experimental.cardinality(test_dataset))\n",
        "\n",
        "\n",
        "    AUTOTUNE = tf.data.AUTOTUNE\n",
        "    train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\n",
        "    if validation_dataset:\n",
        "      validation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)\n",
        "      test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "    data_augmentation = tf.keras.Sequential([\n",
        "      tf.keras.layers.RandomFlip('horizontal'),\n",
        "      tf.keras.layers.RandomRotation(0.2),\n",
        "    ])\n",
        "\n",
        "    preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\n",
        "    # rescale = tf.keras.layers.Rescaling(1./127.5, offset=-1)\n",
        "\n",
        "    # Create the base model from the pre-trained model MobileNet V2\n",
        "    image_shape = image_size + (3,)\n",
        "    base_model = tf.keras.applications.MobileNetV2(input_shape=image_shape,\n",
        "                                               include_top=False,\n",
        "                                               weights='imagenet')\n",
        "\n",
        "    image_batch, label_batch = next(iter(train_dataset))\n",
        "    feature_batch = base_model(image_batch)\n",
        "    # print(feature_batch.shape)\n",
        "\n",
        "\n",
        "    # Feature Extaction\n",
        "    base_model.trainable = False\n",
        "    # Let's take a look at the base model architecture\n",
        "    # base_model.summary()\n",
        "    global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
        "    feature_batch_average = global_average_layer(feature_batch)\n",
        "    # print(feature_batch_average.shape)\n",
        "    prediction_layer = tf.keras.layers.Dense(1)\n",
        "    prediction_batch = prediction_layer(feature_batch_average)\n",
        "    # print(prediction_batch.shape)\n",
        "    inputs = tf.keras.Input(shape=image_shape)\n",
        "    x = data_augmentation(inputs)\n",
        "    x = preprocess_input(x)\n",
        "    x = base_model(x, training=False)\n",
        "    x = global_average_layer(x)\n",
        "    x = tf.keras.layers.Dropout(0.2)(x)\n",
        "    outputs = prediction_layer(x)\n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "    base_learning_rate = 0.0001\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "    # model.summary()\n",
        "    # len(model.trainable_variables)\n",
        "\n",
        "    # if validation_dataset:\n",
        "    #   loss0, accuracy0 = model.evaluate(validation_dataset)\n",
        "    #   print(\"initial loss: {:.2f}\".format(loss0))\n",
        "    #   print(\"initial accuracy: {:.2f}\".format(accuracy0))\n",
        "\n",
        "    history = model.fit(train_dataset,\n",
        "                    epochs=initial_epachs,\n",
        "                    validation_data=validation_dataset)\n",
        "    # acc = history.history['accuracy']\n",
        "    # loss = history.history['loss']\n",
        "    # if validation_dataset:\n",
        "    #   val_acc = history.history['val_accuracy']\n",
        "    #   val_loss = history.history['val_loss']\n",
        "\n",
        "    # Finetune\n",
        "    base_model.trainable = True\n",
        "    # print(\"Number of layers in the base model: \", len(base_model.layers))\n",
        "    fine_tune_at = 100\n",
        "    for layer in base_model.layers[:fine_tune_at]:\n",
        "      layer.trainable =  False\n",
        "    model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              optimizer = tf.keras.optimizers.RMSprop(learning_rate=base_learning_rate/10),\n",
        "              metrics=['accuracy'])\n",
        "    # model.summary()\n",
        "    # len(model.trainable_variables)\n",
        "\n",
        "    total_epochs =  initial_epachs + finetune_epochs\n",
        "\n",
        "    history_fine = model.fit(train_dataset,\n",
        "                         epochs=total_epochs,\n",
        "                         initial_epoch=history.epoch[-1],\n",
        "                         validation_data=validation_dataset)\n",
        "    # acc += history_fine.history['accuracy']\n",
        "    # loss += history_fine.history['loss']\n",
        "    # if validation_dataset:\n",
        "    #   val_acc += history_fine.history['val_accuracy']\n",
        "    #   val_loss += history_fine.history['val_loss']\n",
        "\n",
        "    if test_dataset:\n",
        "      loss, accuracy = model.evaluate(test_dataset)\n",
        "      print('Test accuracy :', accuracy)\n",
        "\n",
        "      # Retrieve a batch of images from the test set\n",
        "      image_batch, label_batch = test_dataset.as_numpy_iterator().next()\n",
        "      predictions = model.predict_on_batch(image_batch).flatten()\n",
        "\n",
        "      # Apply a sigmoid since our model returns logits\n",
        "      predictions = tf.nn.sigmoid(predictions)\n",
        "      predictions = tf.where(predictions < 0.5, 0, 1)\n",
        "\n",
        "      print('Predictions:\\n', predictions.numpy())\n",
        "      print('Labels:\\n', label_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train my Data"
      ],
      "metadata": {
        "id": "RZjE7iZbZhx7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload my Data"
      ],
      "metadata": {
        "id": "jKZ8ClOkZ6Ta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "filename=None\n",
        "for fn in uploaded.keys():\n",
        "  filename = fn  \n",
        "\n",
        "!unzip $filename\n",
        "base, ext = os.path.splitext(filename)\n",
        "train_dir = f'{base}/train'\n",
        "validation_dir = f'{base}/validation'\n",
        "\n",
        "print(f\"train dir: {train_dir}\")\n",
        "print(f\"validation dir: {validation_dir}\")"
      ],
      "metadata": {
        "id": "VL6P_suOZ5SK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "Em5mHPGMeR4q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "INITIAL_EPOCHS = 5\n",
        "FINETUNE_EPOCHS = 20\n",
        "IMG_SIZE = (160, 160)\n",
        "train(train_dir, validation_dir, initial_epachs=INITIAL_EPOCHS, finetune_epochs=FINETUNE_EPOCHS)"
      ],
      "metadata": {
        "id": "1TVF7a28aPOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "iIdQEviGgrK2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
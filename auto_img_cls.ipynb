{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "auto-img-cls.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "yxOYA7t2ZVqw"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Init the AutoML program"
      ],
      "metadata": {
        "id": "yxOYA7t2ZVqw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "0u9W76KIZKGx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def train(train_dir, validation_dir=None, batch_size=32, epochs=1, image_size=(160, 160)):\n",
        "    # Step1: Data prep\n",
        "    train_dataset = tf.keras.utils.image_dataset_from_directory(train_dir,\n",
        "                                                                shuffle=True,\n",
        "                                                                batch_size=batch_size,\n",
        "                                                                image_size=image_size)\n",
        "    class_names = train_dataset.class_names\n",
        "\n",
        "    if validation_dir:\n",
        "        validation_dataset = tf.keras.utils.image_dataset_from_directory(validation_dir,\n",
        "                                                                         shuffle=True,\n",
        "                                                                         batch_size=batch_size,\n",
        "                                                                         image_size=image_size)\n",
        "        batches = tf.data.experimental.cardinality(validation_dataset)\n",
        "        if batches == 1:\n",
        "            test_dataset = validation_dataset\n",
        "        elif batches < 5:\n",
        "            test_dataset = validation_dataset.take(batches // 2)\n",
        "            validation_dataset = validation_dataset.skip(batches // 2)\n",
        "        else:\n",
        "            test_dataset = validation_dataset.take(batches // 5)\n",
        "            validation_dataset = validation_dataset.skip(batches // 5)\n",
        "    else:\n",
        "        test_dataset = None\n",
        "        validation_dataset = None\n",
        "\n",
        "    print('Number of trian batches: %d' %\n",
        "          tf.data.experimental.cardinality(train_dataset))\n",
        "    if validation_dataset:\n",
        "        print('Number of validation batches: %d' %\n",
        "              tf.data.experimental.cardinality(validation_dataset))\n",
        "        print('Number of test batches: %d' %\n",
        "              tf.data.experimental.cardinality(test_dataset))\n",
        "    AUTOTUNE = tf.data.AUTOTUNE\n",
        "    train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\n",
        "    if validation_dataset:\n",
        "        validation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)\n",
        "        test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "    # Step2: Model Architecture\n",
        "    data_augmentation = tf.keras.Sequential([\n",
        "        tf.keras.layers.RandomFlip('horizontal'),\n",
        "        tf.keras.layers.RandomRotation(0.2),\n",
        "    ])\n",
        "    preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\n",
        "    image_shape = image_size + (3,)\n",
        "    base_model = tf.keras.applications.MobileNetV2(input_shape=image_shape,\n",
        "                                                   include_top=False,\n",
        "                                                   weights='imagenet')\n",
        "    base_model.trainable = True\n",
        "    fine_tune_at = 100\n",
        "    for layer in base_model.layers[:fine_tune_at]:\n",
        "        layer.trainable = False\n",
        "\n",
        "    global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
        "    prediction_layer = tf.keras.layers.Dense(len(class_names))\n",
        "    inputs = tf.keras.Input(shape=image_shape)\n",
        "    x = data_augmentation(inputs)\n",
        "    x = preprocess_input(x)\n",
        "    x = base_model(x, training=False)\n",
        "    x = global_average_layer(x)\n",
        "    x = tf.keras.layers.Dropout(0.2)(x)\n",
        "    outputs = prediction_layer(x)\n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "    base_learning_rate = 0.0001\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n",
        "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "                      from_logits=True),\n",
        "                  metrics=['accuracy'])\n",
        "    model.summary()\n",
        "\n",
        "    # Step 3: Start training\n",
        "    history = model.fit(train_dataset,\n",
        "                        epochs=epochs,\n",
        "                        validation_data=validation_dataset)\n",
        "\n",
        "    # Step 4: Evaluation\n",
        "    if test_dataset:\n",
        "        loss, accuracy = model.evaluate(test_dataset)\n",
        "        print('Test accuracy :', accuracy)\n",
        "\n",
        "        # Retrieve a batch of images from the test set\n",
        "        image_batch, label_batch = test_dataset.as_numpy_iterator().next()\n",
        "        predictions = model.predict_on_batch(image_batch)\n",
        "\n",
        "        # Apply softmax and argmax to find the most possible class\n",
        "        predictions = model.predict_on_batch(image_batch)\n",
        "        predictions = tf.nn.softmax(predictions)\n",
        "        predictions = tf.math.argmax(predictions, axis=-1)\n",
        "        print('Predictions:\\n', predictions.numpy())\n",
        "        print('Labels:\\n', label_batch)\n",
        "\n",
        "    return (model, class_names, history)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train my Data"
      ],
      "metadata": {
        "id": "RZjE7iZbZhx7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload my Data"
      ],
      "metadata": {
        "id": "jKZ8ClOkZ6Ta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "filename=None\n",
        "for fn in uploaded.keys():\n",
        "  filename = fn  \n",
        "\n",
        "!unzip $filename\n",
        "base, ext = os.path.splitext(filename)\n",
        "train_dir = f'{base}/train'\n",
        "validation_dir = f'{base}/validation'\n",
        "\n",
        "print(f\"train dir: {train_dir}\")\n",
        "print(f\"validation dir: {validation_dir}\")"
      ],
      "metadata": {
        "id": "VL6P_suOZ5SK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download Data from URL"
      ],
      "metadata": {
        "id": "-eFB1gChYEV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'\n",
        "basename = os.path.basename(_URL)\n",
        "base, ext = os.path.splitext(basename)\n",
        "path_to_zip = tf.keras.utils.get_file(basename, origin=_URL, extract=True)\n",
        "PATH = os.path.join(os.path.dirname(path_to_zip), base)\n",
        "train_dir = os.path.join(PATH, 'train')\n",
        "validation_dir = os.path.join(PATH, 'validation')\n",
        "\n",
        "print(f\"train dir: {train_dir}\")\n",
        "print(f\"validation dir: {validation_dir}\")"
      ],
      "metadata": {
        "id": "AhrmbOaQOGcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "Em5mHPGMeR4q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "EPOCHS = 2\n",
        "IMG_SIZE = (160, 160)\n",
        "model, class_name, history = train(train_dir, validation_dir, epochs=EPOCHS)"
      ],
      "metadata": {
        "id": "1TVF7a28aPOJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}